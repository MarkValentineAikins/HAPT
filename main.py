import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \
    classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings

warnings.filterwarnings('ignore')


class HierarchicalAdaptivePredictionTree:
    """
    Hierarchical Adaptive Prediction Tree (HAPT) Algorithm

    A novel algorithm designed to handle hierarchical healthcare data
    by creating adaptive prediction trees that can adjust to different
    levels of the data hierarchy.
    """

    def __init__(self, max_depth=10, min_samples_split=10, min_samples_leaf=5,
                 hierarchy_levels=3, adaptation_threshold=0.1):
        """
        Initialize HAPT algorithm

        Parameters:
        - max_depth: Maximum depth of individual trees
        - min_samples_split: Minimum samples required to split a node
        - min_samples_leaf: Minimum samples required at leaf node
        - hierarchy_levels: Number of hierarchical levels to consider
        - adaptation_threshold: Threshold for adaptive splitting
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.hierarchy_levels = hierarchy_levels
        self.adaptation_threshold = adaptation_threshold
        self.trees = {}
        self.hierarchy_structure = {}
        self.feature_importance_ = None
        self.classes_ = None

    def get_params(self, deep=True):
        """
        Get parameters for this estimator (required for sklearn compatibility)
        """
        return {
            'max_depth': self.max_depth,
            'min_samples_split': self.min_samples_split,
            'min_samples_leaf': self.min_samples_leaf,
            'hierarchy_levels': self.hierarchy_levels,
            'adaptation_threshold': self.adaptation_threshold
        }

    def set_params(self, **params):
        """
        Set parameters for this estimator (required for sklearn compatibility)
        """
        for key, value in params.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                raise ValueError(f"Invalid parameter {key} for estimator {self.__class__.__name__}")
        return self

    def _identify_hierarchy(self, X, y):
        """
        Identify hierarchical structure in the data
        """
        # Create hierarchy based on feature correlations and target distributions
        hierarchy = {}

        # Level 1: Primary demographic features
        demographic_features = [col for col in X.columns if any(keyword in col.lower()
                                                                for keyword in
                                                                ['age', 'gender', 'region', 'residence'])]

        # Level 2: Health status features
        health_features = [col for col in X.columns if any(keyword in col.lower()
                                                           for keyword in
                                                           ['health', 'disease', 'condition', 'treatment'])]

        # Level 3: Outcome features
        outcome_features = [col for col in X.columns if col not in demographic_features + health_features]

        hierarchy['level_1'] = demographic_features[:min(len(demographic_features), 5)]
        hierarchy['level_2'] = health_features[:min(len(health_features), 8)]
        hierarchy['level_3'] = outcome_features[:min(len(outcome_features), 10)]

        return hierarchy

    def _adaptive_split(self, X_subset, y_subset, level):
        """
        Perform adaptive splitting based on hierarchy level
        """
        if len(X_subset) < self.min_samples_split:
            return None

        # Calculate information gain for adaptive splitting
        best_feature = None
        best_threshold = None
        best_gain = -1

        for feature in X_subset.columns:
            if X_subset[feature].dtype in ['int64', 'float64']:
                # For numerical features
                thresholds = np.percentile(X_subset[feature], [25, 50, 75])
                for threshold in thresholds:
                    left_mask = X_subset[feature] <= threshold
                    right_mask = ~left_mask

                    if sum(left_mask) < self.min_samples_leaf or sum(right_mask) < self.min_samples_leaf:
                        continue

                    # Calculate weighted information gain
                    gain = self._calculate_information_gain(y_subset, y_subset[left_mask], y_subset[right_mask])

                    # Apply hierarchy-based weighting
                    level_weight = 1.0 / (level + 1)  # Higher levels get lower weights
                    weighted_gain = gain * level_weight

                    if weighted_gain > best_gain:
                        best_gain = weighted_gain
                        best_feature = feature
                        best_threshold = threshold
            else:
                # For categorical features
                unique_values = X_subset[feature].unique()
                if len(unique_values) > 1:
                    for value in unique_values:
                        left_mask = X_subset[feature] == value
                        right_mask = ~left_mask

                        if sum(left_mask) < self.min_samples_leaf or sum(right_mask) < self.min_samples_leaf:
                            continue

                        gain = self._calculate_information_gain(y_subset, y_subset[left_mask], y_subset[right_mask])
                        level_weight = 1.0 / (level + 1)
                        weighted_gain = gain * level_weight

                        if weighted_gain > best_gain:
                            best_gain = weighted_gain
                            best_feature = feature
                            best_threshold = value

        return {'feature': best_feature, 'threshold': best_threshold, 'gain': best_gain}

    def _calculate_information_gain(self, parent, left_child, right_child):
        """
        Calculate information gain for a split
        """

        def entropy(y):
            if len(y) == 0:
                return 0
            proportions = np.bincount(y) / len(y)
            return -np.sum([p * np.log2(p) for p in proportions if p > 0])

        parent_entropy = entropy(parent)
        left_weight = len(left_child) / len(parent)
        right_weight = len(right_child) / len(parent)

        weighted_child_entropy = left_weight * entropy(left_child) + right_weight * entropy(right_child)

        return parent_entropy - weighted_child_entropy

    def fit(self, X, y):
        """
        Train the HAPT algorithm
        """
        # Reset internal state for fresh fitting
        self.trees = {}
        self.hierarchy_structure = {}
        self.feature_importance_ = None
        self.classes_ = None

        # Identify hierarchical structure
        self.hierarchy_structure = self._identify_hierarchy(X, y)
        self.classes_ = np.unique(y)

        # Build trees for each hierarchy level
        for level in range(self.hierarchy_levels):
            level_name = f'level_{level + 1}'

            if level_name in self.hierarchy_structure:
                # Select features for this level
                if level == 0:
                    level_features = self.hierarchy_structure[level_name]
                else:
                    # Include features from previous levels
                    prev_features = []
                    for prev_level in range(level):
                        prev_level_name = f'level_{prev_level + 1}'
                        if prev_level_name in self.hierarchy_structure:
                            prev_features.extend(self.hierarchy_structure[prev_level_name])
                    level_features = prev_features + self.hierarchy_structure[level_name]

                # Filter features that exist in X
                level_features = [f for f in level_features if f in X.columns]

                if level_features:
                    X_level = X[level_features]

                    # Create decision tree for this level with adaptive parameters
                    tree = DecisionTreeClassifier(
                        max_depth=max(3, self.max_depth - level),
                        min_samples_split=self.min_samples_split,
                        min_samples_leaf=self.min_samples_leaf,
                        random_state=42
                    )

                    tree.fit(X_level, y)
                    self.trees[level_name] = {
                        'tree': tree,
                        'features': level_features,
                        'weight': 1.0 / (level + 1),  # Higher levels get lower weights
                        'level': level
                    }

        # Calculate feature importance across all levels
        self._calculate_feature_importance(X)

        return self

    def _calculate_feature_importance(self, X):
        """
        Calculate feature importance across all hierarchy levels
        """
        feature_importance = {}

        for level_name, level_data in self.trees.items():
            tree = level_data['tree']
            features = level_data['features']
            weight = level_data['weight']

            # Get feature importance from this level's tree
            if hasattr(tree, 'feature_importances_'):
                for i, feature in enumerate(features):
                    if feature not in feature_importance:
                        feature_importance[feature] = 0
                    feature_importance[feature] += tree.feature_importances_[i] * weight

        # Normalize feature importance
        total_importance = sum(feature_importance.values())
        if total_importance > 0:
            self.feature_importance_ = {k: v / total_importance for k, v in feature_importance.items()}
        else:
            self.feature_importance_ = feature_importance

    def predict(self, X):
        """
        Make predictions using the HAPT algorithm
        """
        predictions = []

        for idx in range(len(X)):
            level_predictions = {}
            level_probabilities = {}

            # Get predictions from each level
            for level_name, level_data in self.trees.items():
                tree = level_data['tree']
                features = level_data['features']
                weight = level_data['weight']

                # Filter features that exist in X
                available_features = [f for f in features if f in X.columns]

                if available_features:
                    X_level = X[available_features].iloc[idx:idx + 1]
                    pred = tree.predict(X_level)[0]
                    prob = tree.predict_proba(X_level)[0]

                    level_predictions[level_name] = pred
                    level_probabilities[level_name] = prob * weight

            # Combine predictions using weighted voting
            if level_probabilities:
                # Sum weighted probabilities
                combined_probs = np.zeros(len(self.classes_))
                total_weight = 0

                for level_name, probs in level_probabilities.items():
                    combined_probs += probs
                    total_weight += self.trees[level_name]['weight']

                if total_weight > 0:
                    combined_probs /= total_weight

                # Get class with highest probability
                final_prediction = self.classes_[np.argmax(combined_probs)]
            else:
                # Fallback to most common class
                final_prediction = self.classes_[0]

            predictions.append(final_prediction)

        return np.array(predictions)

    def predict_proba(self, X):
        """
        Predict class probabilities using the HAPT algorithm
        """
        probabilities = []

        for idx in range(len(X)):
            level_probabilities = {}

            # Get predictions from each level
            for level_name, level_data in self.trees.items():
                tree = level_data['tree']
                features = level_data['features']
                weight = level_data['weight']

                # Filter features that exist in X
                available_features = [f for f in features if f in X.columns]

                if available_features:
                    X_level = X[available_features].iloc[idx:idx + 1]
                    prob = tree.predict_proba(X_level)[0]
                    level_probabilities[level_name] = prob * weight

            # Combine predictions using weighted voting
            if level_probabilities:
                combined_probs = np.zeros(len(self.classes_))
                total_weight = 0

                for level_name, probs in level_probabilities.items():
                    combined_probs += probs
                    total_weight += self.trees[level_name]['weight']

                if total_weight > 0:
                    combined_probs /= total_weight
                else:
                    combined_probs = np.ones(len(self.classes_)) / len(self.classes_)
            else:
                combined_probs = np.ones(len(self.classes_)) / len(self.classes_)

            probabilities.append(combined_probs)

        return np.array(probabilities)


# Model Comparison and Evaluation Class
class ModelComparison:
    """
    Class for comparing HAPT, Decision Tree, and Random Forest algorithms
    """

    def __init__(self):
        self.models = {}
        self.results = {}

    def prepare_data(self, data, target_column):
        """
        Prepare data for model training
        """
        # Separate features and target
        X = data.drop(columns=[target_column])
        y = data[target_column]

        # Handle categorical variables
        categorical_columns = X.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))

        # Handle missing values
        X = X.fillna(X.mean())

        # Encode target variable if categorical
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)

        return X, y

    def train_models(self, X, y, test_size=0.2, random_state=42):
        """
        Train all three models
        """
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        # Initialize models
        models = {
            'HAPT': HierarchicalAdaptivePredictionTree(
                max_depth=10, min_samples_split=10, min_samples_leaf=5,
                hierarchy_levels=3, adaptation_threshold=0.1
            ),
            'Decision Tree': DecisionTreeClassifier(
                max_depth=10, min_samples_split=10, min_samples_leaf=5,
                random_state=42
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=100, max_depth=10, min_samples_split=10,
                min_samples_leaf=5, random_state=42
            )
        }

        # Train models and evaluate
        for name, model in models.items():
            print(f"Training {name}...")

            # Train model
            model.fit(X_train, y_train)

            # Make predictions
            y_pred = model.predict(X_test)

            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')

            # Cross-validation (with error handling for HAPT)
            try:
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            except Exception as e:
                print(f"Cross-validation failed for {name}: {e}")
                # Manual cross-validation for HAPT
                if name == 'HAPT':
                    cv_scores = []
                    from sklearn.model_selection import KFold
                    kfold = KFold(n_splits=5, shuffle=True, random_state=42)

                    for train_idx, val_idx in kfold.split(X_train):
                        X_train_fold = X_train.iloc[train_idx]
                        X_val_fold = X_train.iloc[val_idx]
                        y_train_fold = y_train.iloc[train_idx]
                        y_val_fold = y_train.iloc[val_idx]

                        # Create a new model instance for this fold
                        fold_model = HierarchicalAdaptivePredictionTree(
                            max_depth=10, min_samples_split=10, min_samples_leaf=5,
                            hierarchy_levels=3, adaptation_threshold=0.1
                        )
                        fold_model.fit(X_train_fold, y_train_fold)
                        y_pred_fold = fold_model.predict(X_val_fold)
                        fold_accuracy = accuracy_score(y_val_fold, y_pred_fold)
                        cv_scores.append(fold_accuracy)

                    cv_scores = np.array(cv_scores)
                    cv_mean = cv_scores.mean()
                    cv_std = cv_scores.std()
                else:
                    cv_mean = accuracy  # Fallback to test accuracy
                    cv_std = 0.0

            # Store results
            self.models[name] = model
            self.results[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'predictions': y_pred,
                'actual': y_test
            }

            print(f"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, CV: {cv_mean:.4f}Â±{cv_std:.4f}")

    def statistical_significance_test(self):
        """
        Perform statistical significance test between models
        """
        results_df = pd.DataFrame(self.results).T

        # Perform pairwise t-tests
        models = list(self.results.keys())
        significance_results = {}

        for i in range(len(models)):
            for j in range(i + 1, len(models)):
                model1, model2 = models[i], models[j]

                # Get cross-validation scores for both models
                # For simplicity, we'll use the stored CV results
                # In a real scenario, you'd want to run multiple CV folds
                acc1 = self.results[model1]['accuracy']
                acc2 = self.results[model2]['accuracy']

                # Simplified significance test (in practice, use proper CV scores)
                diff = abs(acc1 - acc2)
                significance_results[f'{model1} vs {model2}'] = {
                    'difference': diff,
                    'significant': diff > 0.05  # Simplified threshold
                }

        return significance_results

    def generate_report(self):
        """
        Generate comprehensive comparison report
        """
        print("\n" + "=" * 60)
        print("MODEL COMPARISON REPORT")
        print("=" * 60)

        # Results table
        results_df = pd.DataFrame(self.results).T
        print("\nPerformance Metrics:")
        print(results_df[['accuracy', 'precision', 'recall', 'f1_score', 'cv_mean', 'cv_std']].round(4))

        # Statistical significance
        print("\nStatistical Significance Test:")
        significance = self.statistical_significance_test()
        for comparison, result in significance.items():
            print(f"{comparison}: Difference = {result['difference']:.4f}, "
                  f"Significant = {result['significant']}")

        # Best performing model
        best_model = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])
        print(f"\nBest Performing Model: {best_model}")
        print(f"Accuracy: {self.results[best_model]['accuracy']:.4f}")

        return results_df


# Example usage function
def run_hapt_analysis(data, target_column):
    """
    Run complete HAPT analysis
    """
    # Initialize comparison
    comparison = ModelComparison()

    # Prepare data
    X, y = comparison.prepare_data(data, target_column)

    # Train models
    comparison.train_models(X, y)

    # Generate report
    results_df = comparison.generate_report()

    return comparison, results_df


# Sample data generation for testing (since actual DHS data is not available)
def generate_sample_healthcare_data(n_samples=1000):
    """
    Generate sample healthcare data for testing
    """
    np.random.seed(42)

    # Generate sample data
    data = {
        'age': np.random.randint(18, 80, n_samples),
        'gender': np.random.choice(['Male', 'Female'], n_samples),
        'region': np.random.choice(['Northern', 'Southern', 'Eastern', 'Western'], n_samples),
        'residence': np.random.choice(['Urban', 'Rural'], n_samples),
        'education': np.random.choice(['None', 'Primary', 'Secondary', 'Higher'], n_samples),
        'income_level': np.random.choice(['Low', 'Medium', 'High'], n_samples),
        'bmi': np.random.normal(25, 5, n_samples),
        'blood_pressure': np.random.choice(['Normal', 'High', 'Low'], n_samples),
        'diabetes': np.random.choice(['Yes', 'No'], n_samples),
        'smoking': np.random.choice(['Yes', 'No'], n_samples),
        'exercise': np.random.choice(['Regular', 'Occasional', 'None'], n_samples),
        'family_history': np.random.choice(['Yes', 'No'], n_samples),
        'health_insurance': np.random.choice(['Yes', 'No'], n_samples),
        'hospital_visits': np.random.randint(0, 10, n_samples),
        'medication_adherence': np.random.choice(['High', 'Medium', 'Low'], n_samples)
    }

    # Create target variable (health outcome)
    health_outcome = []
    for i in range(n_samples):
        # Simple logic to create realistic health outcomes
        score = 0
        if data['age'][i] > 60:
            score += 2
        if data['bmi'][i] > 30:
            score += 1
        if data['blood_pressure'][i] == 'High':
            score += 2
        if data['diabetes'][i] == 'Yes':
            score += 2
        if data['smoking'][i] == 'Yes':
            score += 1
        if data['exercise'][i] == 'None':
            score += 1
        if data['family_history'][i] == 'Yes':
            score += 1

        if score >= 5:
            health_outcome.append('Poor')
        elif score >= 3:
            health_outcome.append('Fair')
        else:
            health_outcome.append('Good')

    data['health_outcome'] = health_outcome

    return pd.DataFrame(data)


# Main execution
if __name__ == "__main__":
    # Generate sample data
    print("Generating sample healthcare data...")
    sample_data = generate_sample_healthcare_data(1000)

    print("Sample data shape:", sample_data.shape)
    print("\nSample data preview:")
    print(sample_data.head())

    # Run analysis
    print("\nRunning HAPT analysis...")
    comparison, results = run_hapt_analysis(sample_data, 'health_outcome')

    # Additional analysis
    print("\n" + "=" * 60)
    print("ADDITIONAL ANALYSIS")
    print("=" * 60)

    # Feature importance for HAPT
    if 'HAPT' in comparison.models:
        hapt_model = comparison.models['HAPT']
        if hasattr(hapt_model, 'feature_importance_') and hapt_model.feature_importance_:
            print("\nHAPT Feature Importance:")
            sorted_features = sorted(hapt_model.feature_importance_.items(),
                                     key=lambda x: x[1], reverse=True)
            for feature, importance in sorted_features[:10]:
                print(f"{feature}: {importance:.4f}")

    print("\nAnalysis complete!")
